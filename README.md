# ETL Pipeline Project for UK Food Standards Agency Ratings Data1
![may-gauthier-RkRbL1Exqwc-unsplash](https://github.com/CLizardi/etl_pipeline_project/assets/52866379/64414e83-dda6-4d75-8f4f-875a25b8ca74)

# Introduction:
This ETL Pipeline project was completed as a part of a challenge to evaluate the food hygiene rating data collected by the UK Food Standards Agency. The goal of the project was to assist food magazine editors in identifying establishments for future articles. The project involved three main parts: setting up the database and Jupyter Notebook, updating the database by adding new data, removing unwanted data, and cleaning up existing data, and conducting exploratory analysis to answer specific questions related to the data.

# Project Overview:
The project began by setting up the database and Jupyter Notebook using the PyMongo and Pretty Print libraries. The data was imported from the establishments.json file into the uk_food database and the establishments collection. The data was then modified by adding a new restaurant, updating the BusinessTypeID, removing establishments within the Dover Local Authority, and converting certain string values to decimal or integer values. Finally, exploratory analysis was conducted to answer specific questions about the data, including identifying establishments with a hygiene score of 20, identifying establishments in London with a rating value of 4 or higher, identifying the top 5 establishments with a rating value of 5 and sorted by lowest hygiene score, nearest to the new restaurant added, and identifying the number of establishments with a hygiene score of 0 in each local authority area, sorted by count.

# What I Did:
I followed the instructions provided and completed the ETL Pipeline project. Specifically, I imported data from the establishments.json file into the uk_food database and the establishments collection. I modified the data by adding a new restaurant, updating the BusinessTypeID, removing establishments within the Dover Local Authority, and converting certain string values to decimal or integer values. Finally, I conducted exploratory analysis to answer specific questions about the data.

# Tools Used:
For this project, I used PyMongo and Pretty Print libraries to set up the database and Jupyter Notebook, and to manipulate the data. I also used Pandas and Matplotlib libraries for data visualization and analysis.

# What I Learned:
This project provided an opportunity to work with NoSQL databases, specifically MongoDB. I learned how to import data into a database, modify it, and retrieve specific information. I also learned how to use PyMongo and Pretty Print libraries to manipulate data, Pandas and Matplotlib libraries for data visualization and analysis, and Jupyter Notebook for documentation and presentation. Finally, I learned how to write clear and concise comments to make my code more understandable to other developers.

# Conclusion:
In conclusion, this ETL Pipeline project provided a comprehensive understanding of working with NoSQL databases, specifically MongoDB. The project involved importing data, modifying it, and conducting exploratory analysis to answer specific questions. The project helped in gaining experience with various tools and libraries such as PyMongo, Pretty Print, Pandas, Matplotlib, and Jupyter Notebook. The project also helped in enhancing commenting skills to make the code more readable and understandable for other developers.
